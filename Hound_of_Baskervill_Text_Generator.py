
# coding: utf-8

# # GPT-2 Style Text Generation
# GPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset of 8 million web pages. GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some text. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains. GPT-2 is a direct scale-up of GPT, with more than 10X the parameters and trained on more than 10X the amount of data.
# 
# However, as we don't have such huge corpus of data and we have limitations on the computing resources, I have used my all time favourite novel 'the Hound of the Baskervilles' by Sir Arthur Conan Doyle.
# 
# ## How does it work?
# 
# If you give a sample sentence as input, it will try to add more text in a similar context.


def read_file(fp):
    with open(fp) as f:
        str_text = f.read()
    return str_text


stories = read_file('/home/faceopen/pdf-stories/Large_Text.text')

import spacy
nlp = spacy.load('en',disable=['parser', 'tagger','ner'])

nlp.max_length = 2779157

def separate_punc(doc_text):
    return [token.text.lower() for token in nlp(doc_text) if token.text not in '\n\n \n\n\n!"-#$%&()--.*+,-/:;<=>?@[\\]^_`{|}~\t\n ']

tokens = separate_punc(stories)

# organize into sequences of tokens
train_len = 25+1 # 50 training words , then one target word

# Empty list of sequences
text_sequences = []

for i in range(train_len, len(tokens)):
    
    # Grab train_len# amount of characters
    seq = tokens[i-train_len:i]
    
    # Add to list of sequences
    text_sequences.append(seq)


from keras.preprocessing.text import Tokenizer
tokenizer = Tokenizer()
tokenizer.fit_on_texts(text_sequences)
sequences = tokenizer.texts_to_sequences(text_sequences)

for i in sequences[0]:
    print(f'{i} : {tokenizer.index_word[i]}')

vocabulary_size = len(tokenizer.word_counts)

import numpy as np
sequences = np.array(sequences)
sequences[:,-1].shape

import keras
from keras.models import Sequential
from keras.layers import Dense,LSTM,Embedding

def create_model(vocabulary_size, seq_len):
    model = Sequential()
    model.add(Embedding(vocabulary_size, 18, input_length=seq_len))
    model.add(LSTM(150, return_sequences=True))
    model.add(LSTM(150))
    model.add(Dense(150, activation='relu'))

    model.add(Dense(vocabulary_size, activation='softmax'))
    
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
   
    model.summary()
    
    return model

from keras.utils import to_categorical
X = sequences[:,:-1]
y = sequences[:,-1]
y = to_categorical(y)

seq_len = X.shape[1]

# define model
model = create_model(vocabulary_size+1, seq_len)

from pickle import dump,load
model.fit(X, y, batch_size=128, epochs=233,verbose=1)

from random import randint
from pickle import load
from keras.models import load_model
from keras.preprocessing.sequence import pad_sequences

def generate_text(model, tokenizer, seq_len, seed_text, num_gen_words):
    '''
    INPUTS:
    model : model that was trained on text data
    tokenizer : tokenizer that was fit on text data
    seq_len : length of training sequence
    seed_text : raw string text to serve as the seed
    num_gen_words : number of words to be generated by model
    '''
    
    # Final Output
    output_text = []
    
    # Intial Seed Sequence
    input_text = seed_text
    
    # Create num_gen_words
    for i in range(num_gen_words):
        
        # Take the input text string and encode it to a sequence
        encoded_text = tokenizer.texts_to_sequences([input_text])[0]
        
        # Pad sequences to our trained rate (50 words in the video)
        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')
        
        # Predict Class Probabilities for each word
        pred_word_ind = model.predict_classes(pad_encoded, verbose=0)[0]
        
        # Grab word
        pred_word = tokenizer.index_word[pred_word_ind] 
        
        # Update the sequence of input text (shifting one over with the new word)
        input_text += ' ' + pred_word
        
        output_text.append(pred_word)
        
    # Make it look like a sentence.
    return seed_text + '  '.join(output_text)

seed_text = 'I had a hound.'

generate_text(model,tokenizer,seq_len,seed_text=seed_text,num_gen_words=50)

seed_text = 'Raj is a hard worker'
generate_text(model,tokenizer,seq_len,seed_text=seed_text,num_gen_words=50)
seed_text = 'The walk on the moor'
generate_text(model,tokenizer,seq_len,seed_text=seed_text,num_gen_words=50)
seed_text = 'A visit to baskervill'
generate_text(model,tokenizer,seq_len,seed_text=seed_text,num_gen_words=50)
# save the model to file
model.save('HoB.h5')
# save the tokenizer
dump(tokenizer, open('HoB', 'wb'))
